# 人工智能学习路线
- 作者：codehackfox@gmail.com
- 时间：2019-03-10 18:17:19


>## 0x00、数学基础
* 线性代数：如何将研究对象形式化
    * 事实上，线性代数不仅仅是人工智能的基础，更是现代数学和以现代数学作为主要分析方法的众多学科的基础。从量子力学到图像处理都离不开向量和矩阵的使用。而在向量和矩阵背后，线性代数的核心意义在于提供了⼀种看待世界的抽象视角：万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察。
    * 着重于抽象概念的解释而非具体的数学公式来看，线性代数要点如下：线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态的特性；向量的实质是 n 维线性空间中的静止点；线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示；矩阵的特征值和特征向量描述了变化的速度与方向。
    * 总之，线性代数之于人工智能如同加法之于高等数学，是一个基础的工具集。
* 概率论：如何描述统计规律？
    * 除了线性代数之外，概率论也是人工智能研究中必备的数学基础。随着连接主义学派的兴起，概率统计已经取代了数理逻辑，成为人工智能研究的主流工具。在数据爆炸式增长和计算力指数化增强的今天，概率论已经在机器学习中扮演了核心角色。
    * 同线性代数一样，概率论也代表了一种看待世界的方式，其关注的焦点是无处不在的可能性。频率学派认为先验分布是固定的，模型参数要靠最大似然估计计算；贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算；正态分布是最重要的一种随机变量的分布。
* 数理统计：如何以小见大？
    * 在人工智能的研究中，数理统计同样不可或缺。基础的统计理论有助于对机器学习的算法和数据挖掘的结果做出解释，只有做出合理的解读，数据的价值才能够体现。数理统计根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断。
    * 虽然数理统计以概率论为理论基础，但两者之间存在方法上的本质区别。概率论作用的前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断。
    * 用一句不严谨但直观的话讲：数理统计可以看成是逆向的概率论。 数理统计的任务是根据可观察的样本反过来推断总体的性质；推断的工具是统计量，统计量是样本的函数，是个随机变量；参数估计通过随机抽取的样本来估计总体分布的未知参数，包括点估计和区间估计；假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用于估计机器学习模型的泛化错误率。
* 最优化理论： 如何找到最优解？
    * 本质上讲，人工智能的目标就是最优化：在复杂环境与多体交互中做出最优决策。几乎所有的人工智能问题最后都会归结为一个优化问题的求解，因而最优化理论同样是人工智能必备的基础知识。最优化理论研究的问题是判定给定目标函数的最大值（最小值）是否存在，并找到令目标函数取到最大值 (最小值) 的数值。 如果把给定的目标函数看成一座山脉，最优化的过程就是判断顶峰的位置并找到到达顶峰路径的过程。
    * 通常情况下，最优化问题是在无约束情况下求解给定目标函数的最小值；在线性搜索中，确定寻找最小值时的搜索方向需要使用目标函数的一阶导数和二阶导数；置信域算法的思想是先确定搜索步长，再确定搜索方向；以人工神经网络为代表的启发式算法是另外一类重要的优化方法。
* 信息论：如何定量度量不确定性？
    * 近年来的科学研究不断证实，不确定性就是客观世界的本质属性。换句话说，上帝还真就掷骰子。不确定性的世界只能使用概率模型来描述，这促成了信息论的诞生。
    * 信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。
    * 总之，信息论处理的是客观世界中的不确定性；条件熵和信息增益是分类问题中的重要参数；KL 散度用于描述两个不同概率分布之间的差异；最大熵原理是分类问题汇总的常用准则。
* 形式逻辑：如何实现抽象推理？
    * 1956 年召开的达特茅斯会议宣告了人工智能的诞生。在人工智能的襁褓期，各位奠基者们，包括约翰·麦卡锡、赫伯特·西蒙、马文·闵斯基等未来的图灵奖得主，他们的愿景是让“具备抽象思考能力的程序解释合成的物质如何能够拥有人类的心智。”通俗地说，理想的人工智能应该具有抽象意义上的学习、推理与归纳能力，其通用性将远远强于解决国际象棋或是围棋等具体问题的算法。
    * 如果将认知过程定义为对符号的逻辑运算，人工智能的基础就是形式逻辑；谓词逻辑是知识表示的主要方法；基于谓词逻辑系统可以实现具有自动推理能力的人工智能；不完备性定理向“认知的本质是计算”这一人工智能的基本理念提出挑战。


>## 0x01、机器学习主要方法
* 机器学习概述：如何让计算机识别特征
* 线性回归：如何拟合线性模型？
* 朴素贝叶斯分类：如何利用后验概率
* 逻辑回归：如何利用似然函数
* 决策树方法：如何利用信息增益
* 支持向量机：如何在特征空间上分类
* 集成学习：如何整合优化
* 聚类：如何实现无监督学习
* 降维学习：如何抓大放小


>## 0x02、人工神经网络
* 神经网络的生理学依据：如何模拟人类认知？
* 神经网络的基本单元：如何构造人工神经网络？
* 多层神经网络：如何解决复杂问题？
* 前馈与反向传播：如何用神经网络实现优化？
* 自组织神经网络：如何用神经网络实现无监督学习？
* 模糊神经网络：如何用神经网络实现逻辑功能？


>## 0x03、深度学习
* 深度学习概述：如何让人工神经网络物尽其用？
* 深度前馈网络：如何实现最佳近似？
* 深度学习中的正则化：如何抑制过拟合？
* 深度模型优化：如何提升学习效率？
* 自动编码器：如何实现生成式建模？
* 深度强化学习：如何实现从数据到决策？
* 
* 深度信念网络：如何充分利用隐藏单元？
* 卷积神经网络：如何高效处理网格化数据？
* 递归神经网络：如何高效处理序列数据？
* 生成式对抗网络：如何让神经网络自行优化？
* 长短期记忆神经网络：如何在神经网络中引入记忆？
* 
* 贝叶斯网络：如何利用有向概率图？
* 马尔可夫随扌厼如何利用无向概率图？
* 迁移学习：如何基于小数据学习？
* 集群智能：如何让智能涌现？


>## 0x04、应用场景
* 计算机视觉：如何让人工智能会“看”？
* 语音识别：如何让人工智能会“听”？
* 对话系统：如何让人工智能会“说”？
* 机器翻译：如何让人工智能会“想”？

>## 0x05、基础算法：
* SVM简介；
* 带松弛变量的SVM模型: CSVM；
* 对偶问题；
* 核方法；
* 支持向量回归：SVR；
* Scikit-Learn中的SVM；
* SVM案例分析：Otto商品分类；
* 决策树；
* Scikit-Learn中的决策树模型；
* 决策树案例分析：Otto商品分类；
* Bagging和随机森林；
* Scikit-Learn中的随机森林模型；
* 随机森林案例分析：Otto商品分类；
* Adaboost；
* GBM；
* Scikit-Learn中的GBM；
* XGBoost原理；
* XGBoost工具包使用指南；
* XGBoost的Scikit-Learn接口；
* XGBoost案例分析：Otto商品分类；
* LightGBM原理；
* LightGBM使用指南；
* LightGBM案例分析：Otto商品分析；
* PCA降维原理；
* Scikit-Learn中的PCA；
* t-SNE；
* Scikit-Learn中的 t-SNE；
* 降维案例分析：Otto商品数据降维分析；
* 聚类简介；
* KMean聚类算法；
* Scikit-Learn中的 KMean聚类；
* 聚类案例分析：Event聚类；
* 推荐系统简介；
* 基于内容的推荐；
* 基于用户的协同过滤；
* 基于物品的协同过滤；
* 基于矩阵分解的协同过滤；
* 协同过滤推荐案例分析：MovieLens电影推荐；
* CTR预估简介；
* FTRL模型；
* FM与FFM；
* GBDT；
* Wide and Deep Learning模型；
* CTR案例分析：Criteo CTR预估；



